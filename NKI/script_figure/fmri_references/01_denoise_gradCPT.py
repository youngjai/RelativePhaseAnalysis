import pandas as pd
from glob import glob
import numpy as np
import os
from numpy.polynomial.legendre import Legendre
import scipy.linalg as la
import nibabel as nib
import pickle
import os
import random
import matplotlib.pyplot as plt

#%%

def seed_everything(seed: int = 42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

#%%
seed = 0
seed_everything(seed)


#%%
# study info
study_Name='gradCPT'  # study name = folder name
task = ['CB_run-1', 'EC_run-1', 'EO_run-1', 'GRAD_run-1', 'GRAD_run-2', 'GRAD_run-3', 'IMAG_run-1', 'IMAG_run-2',
        'IMGRAD_run-1', 'IMGRAD_run-2']
dataDir = '/media/eunheeji/78F06404F063C6CA/gradCPT_2024/1_prep/derivatives'
resultDir = '/media/eunheeji/78F06404F063C6CA/gradCPT_2024/5_HMM_analysis'
#TR=750

startTR = 0 # 앞에 (앞에 5tr cut)
EndTR = 0

# subject infor

sub_gradCPT = ['001', '002', '003', '004', '005', '006', '007', '008', '009', '010',
               '011', '012', '013', '014', '015', '016', '017', '018', '019', '020',
               '021', '022', '023', '024', '025', '026', '027', '028']

subj_N =  len(sub_gradCPT)
print('#subject N: ', len(sub_gradCPT))


#%%
# TR check for Task

subj_task_TR_ = {}
for i, sj in enumerate(sub_gradCPT):
    subj_task_TR_[sj] = []
    for t, tname in enumerate(task):
        subj_task_TR_[sj] = np.zeros((1, len(task)))

for i, fname in enumerate(sub_gradCPT):
    print('sub-' + fname)
    for t, tname in enumerate(task):
        path = dataDir + f'/sub-{fname}/func/sub-{fname}_task-{tname}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'
        try:
            fmri_data = nib.load(path).get_fdata()
            print('original: ', fmri_data.shape)

            subj_task_TR_[fname][0, t] = fmri_data.shape[3]
            print(f"TR for sub-00{fname} task-{tname}     " + str(fmri_data.shape[3]))

        except Exception as e:
            print(f"Error processing sub-{fname}: {e}")

#%%
# 각 TASK에 대한 바 그래프 그리기
fig, axs = plt.subplots(len(subj_task_TR_['001'][0]), 1, figsize=(15, 2 * len(subj_task_TR_['001'][0])))
bar_width = 0.2  # 막대 너비
subj_count = len(subj_task_TR_)

for t in range(len(subj_task_TR_['001'][0])):  # TASK의 개수만큼 반복
    ax = axs[t]
    for i, (subj, tr_data) in enumerate(subj_task_TR_.items()):
        x_pos = i + bar_width * (t - (len(subj_task_TR_['001'][0]) - 1) / 2)  # 수정된 부분
        bar = ax.bar(x_pos, tr_data[0, t], width=bar_width, label=f'sub-{subj}')
        height = bar.patches[0].get_height()
        ax.annotate(f'{height}',
                    xy=(x_pos + bar_width / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

    # 그래프 설정
    ax.set_xlabel('Subjects')
    ax.set_ylabel('Number of TRs')
    ax.set_ylim(0, 200)  # 원하는 y-axis 범위 설정
    ax.set_title(f'Task : ' + task[t])
    ax.set_xticks(np.arange(subj_count))
    ax.set_xticklabels([f'{subj}' for subj in subj_task_TR_])
    #ax.legend()

# 전체 그래프 레이아웃 조절
plt.tight_layout()
plt.show()

#%%
def load_confounds(subject, tname):
    #confounds = f'fmriprep/{subject}/func/{subject}_task-{task}_run-1_desc-confounds_timeseries.tsv'
    confounds = f'/media/eunheeji/78F06404F063C6CA/gradCPT_2024/1_prep/derivatives/sub-{subject}/func/sub-{subject}_task-{tname}_desc-confounds_timeseries.tsv'
    df = pd.read_csv(confounds, sep='\t')
    return df

def extract_cols(df):
    cols = ['framewise_displacement']
    return df[cols]

def make_poly_regressors(n_samples, order=2):
    # mean
    X = np.ones((n_samples, 1))
    for d in range(order):
        poly = Legendre.basis(d + 1)
        poly_trend = poly(np.linspace(-1, 1, n_samples))
        X = np.hstack((X, poly_trend[:, None]))
    return X

def clean_data(data, confounds):
    """Clean data by regressing out the following nuisance regressors:
    - six motion parameters and their derivatives
    - global signal
    - framewise displacement
    - six aCompCor components
    - polynomial regressors up to second order
    Parameters
    ----------
    data : array of shape (n_volumes, n_features)
        flattened EPI data
    confounds : pandas Dataframe
        dataframe containing confounds generated by fmriprepp
    Returns
    -------
    data_clean : array of shape (n_volumes, n_features)
        denoised data
    """
    # make predictor matrix using confounds computed by fmriprep
    columns = [
        'global_signal',
        'framewise_displacement',
        'trans_x', 'trans_x_derivative1',
        'trans_y', 'trans_y_derivative1',
        'trans_z', 'trans_z_derivative1',
        'rot_x', 'rot_x_derivative1',
        'rot_y', 'rot_y_derivative1',
        'rot_z', 'rot_z_derivative1',
    ]
    # compcor
    n_comp_cor = 6
    columns += [f"a_comp_cor_{c:02d}" for c in range(n_comp_cor)]
    X = confounds[columns].values
    X = X[startTR:TR-EndTR, :]   # ?? start TR point : TR-end TR
    #TR - startTR - EndTR
    # remove nans
    X[np.isnan(X)] = 0.
    # add polynomial components
    n_samples = X.shape[0]
    X = np.hstack((X, make_poly_regressors(n_samples, order=2)))

    # time to clean up
    # center the data first and store the mean
    data_mean = data.mean(0)
    data = data - data_mean
    coef, _, _, _ = la.lstsq(X, data)  # compute the Least-squares solution)
    # remove trends and add back mean of the data
    data_clean = data - X.dot(coef) + data_mean
    return data_clean


#%%

sub_gradCPT_clean_ = {}
sub_outlierInfo_ = {}


for t, tname in enumerate(task):
    sub_gradCPT_clean_[tname] = []
    sub_outlierInfo_[tname] = []
    for i, subj in enumerate(sub_gradCPT):
        try:

            TR = int(subj_task_TR_[subj][0,t])

            df = load_confounds(subj, tname)
            df = extract_cols(df)
            fd = np.array(df['framewise_displacement'].values.tolist())[startTR:TR-EndTR]

            framewise_displacement = fd
            # n_outliers.append(len([c for c in df.columns if 'motion' in c]))
            percent_motion_outliers = np.sum(framewise_displacement > 0.5) / (TR-EndTR)
            print(subj, ': ', percent_motion_outliers)
            sub_outlierInfo_[tname].append(percent_motion_outliers)

            if percent_motion_outliers < 0.10:  # outlier threshold 10%
                sub_gradCPT_clean_[tname].append(subj)

            # save clean subject list
            with open(resultDir + f'/{tname}_sub_gradCPT_Cri5.pkl', 'wb') as f:
                pickle.dump(sub_gradCPT_clean_[tname], f)
            with open(resultDir + f'/{tname}_sub_gradCPT_Cri5_outliers.pkl', 'wb') as f:
                pickle.dump(sub_outlierInfo_[tname], f)
            with open(resultDir + f'/{tname}_sub_gradCPT_all.pkl', 'wb') as f:
                pickle.dump(sub_gradCPT, f)
        except Exception as e:
            print(f"Error processing sub-{subj}: {e}")

    print('task:', tname, 'N : ', len(sub_gradCPT_clean_[tname]), 'list: ', sub_gradCPT_clean_[tname])

#%%
# load mni mask
mni_mask = nib.load(f'/media/eunheeji/78F06404F063C6CA/gradCPT_2024/6_atlas/gradCPT_mni_mask.nii.gz').get_fdata()
mni_image = nib.load(f'/media/eunheeji/78F06404F063C6CA/gradCPT_2024/6_atlas/gradCPT_mni_mask.nii.gz')

for t, tname in enumerate(task):
    for i, fname in enumerate(sub_gradCPT_clean_[tname]):  # selected subj
        print('+++++++++', fname, '+++++++++')
        TR = int(subj_task_TR_[fname][0, t])

        try:

            path = dataDir + f'/sub-{fname}/func/sub-{fname}_task-{tname}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'
            fmri_data = nib.load(path).get_fdata()
            print('original: ', fmri_data.shape)
            fmri_data = fmri_data[mni_mask == 1, :].T   # masked
            print('masked: ', fmri_data.shape)
            fmri_data = fmri_data[startTR:TR-EndTR, :]  # should be (TR-stratTR-EndTR, 139485)
            print('trim: ', fmri_data.shape)

            fmri_compounds = load_confounds(fname, tname)
            fmri_clean = clean_data(fmri_data, fmri_compounds)
            ffmri_clean = fmri_clean.astype(np.float32)

            new_image = np.zeros(mni_mask.shape + (TR-startTR-EndTR,), dtype=np.float32)
            new_image[mni_mask == 1, :] = fmri_clean.T

            nib.save(nib.Nifti1Image(new_image, mni_image.affine), resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}.nii.gz')
        except Exception as e:
            print(f"Error processing sub-{subj}_task-{tname}: {e}")

#%%
# Additional preprocessing step
import subprocess as sp
import sys
import os

mask_fname = '/media/eunheeji/78F06404F063C6CA/gradCPT_2024/6_atlas/gradCPT_mni_mask.nii.gz'
FWHM = 8
for t, tname in enumerate(task):
    for i, fname in enumerate(sub_gradCPT_clean_[tname]):
        print(i, fname)
        input_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}.nii.gz'
        mean_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}_mean.nii.gz'
        SC_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}_scale.nii.gz'
        SM_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}_smoothing.nii.gz'
        SM_mean_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}_smoothing_mean.nii.gz'
        SM_SC_fname = resultDir + f'/gradCPT_adddenoised/sub-{fname}_task-{tname}_smoothing_scale.nii.gz'

        #sp.call(f'rm {mean_fname} {SC_fname} {SM_fname} {SM_mean_fname} {SM_SC_fname}', shell=True)

        # Scaling
        sp.call(f"3dTstat -prefix {mean_fname} {input_fname} '-overwrite'", shell=True)
        sp.call(
            f"3dcalc -a {input_fname} -b {mean_fname} -c {mask_fname} -expr 'c * min(200, a/b*100)*step(a)*step(b)' -prefix {SC_fname}",
            shell=True)

        # Spatial Smoothing
        sp.call(f'3dmerge -quiet -1blur_fwhm {FWHM} -doall -prefix {SM_fname} {input_fname}', shell=True)

        # Scaling
        sp.call(f"3dTstat -prefix {SM_mean_fname} {SM_fname} '-overwrite'", shell=True)
        sp.call(
            f"3dcalc -a {SM_fname} -b {SM_mean_fname} -c {mask_fname} -expr 'c * min(200, a/b*100)*step(a)*step(b)' -prefix {SM_SC_fname}",
            shell=True)
